{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "cfg = yaml.full_load(open(os.getcwd() + \"/config.yml\", 'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset file paths and labels\n",
    "data = {}\n",
    "data['TRAIN'] = pd.read_csv(cfg['PATHS']['TRAIN_SET'])\n",
    "data['VAL'] = pd.read_csv(cfg['PATHS']['VAL_SET'])\n",
    "data['TEST'] = pd.read_csv(cfg['PATHS']['TEST_SET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "# Set callbacks.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=cfg['TRAIN']['PATIENCE'], mode='min', restore_best_weights=True)\n",
    "callbacks = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.metrics import Metric, Precision, Recall\n",
    "from tensorflow.python.keras.utils import metrics_utils\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras.utils.generic_utils import to_list\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "class F1Score(Metric):\n",
    "    '''\n",
    "    Custom tf.keras metric that calculates the F1 Score\n",
    "    '''\n",
    "\n",
    "    def __init__(self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None):\n",
    "        '''\n",
    "        Creates an instance of the  F1Score class\n",
    "        :param thresholds: A float value or a python list/tuple of float threshold values in [0, 1].\n",
    "        :param top_k: An int value specifying the top-k predictions to consider when calculating precision\n",
    "        :param class_id: Integer class ID for which we want binary metrics. This must be in the half-open interval\n",
    "                `[0, num_classes)`, where `num_classes` is the last dimension of predictions\n",
    "        :param name: string name of the metric instance\n",
    "        :param dtype: data type of the metric result\n",
    "        '''\n",
    "        super(F1Score, self).__init__(name=name, dtype=dtype)\n",
    "        self.init_thresholds = thresholds\n",
    "        self.top_k = top_k\n",
    "        self.class_id = class_id\n",
    "\n",
    "        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n",
    "        self.thresholds = metrics_utils.parse_init_thresholds(\n",
    "            thresholds, default_threshold=default_threshold)\n",
    "        self.true_positives = self.add_weight('true_positives', shape=(len(self.thresholds),),\n",
    "                                              initializer=init_ops.zeros_initializer)\n",
    "        self.false_positives = self.add_weight('false_positives', shape=(len(self.thresholds),),\n",
    "                                               initializer=init_ops.zeros_initializer)\n",
    "        self.false_negatives = self.add_weight('false_negatives', shape=(len(self.thresholds),),\n",
    "                                               initializer=init_ops.zeros_initializer)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        '''\n",
    "        Accumulates true positive, false positive and false negative statistics.\n",
    "        :param y_true: The ground truth values, with the same dimensions as `y_pred`. Will be cast to `bool`\n",
    "        :param y_pred: The predicted values. Each element must be in the range `[0, 1]`\n",
    "        :param sample_weight: Weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0,\n",
    "               or the same rank as `y_true`, and must be broadcastable to `y_true`\n",
    "        :return: Update operation\n",
    "        '''\n",
    "        metrics_utils.update_confusion_matrix_variables(\n",
    "            {\n",
    "                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n",
    "                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,\n",
    "                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives\n",
    "            },\n",
    "            y_true, y_pred, thresholds=self.thresholds, top_k=self.top_k, class_id=self.class_id,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "    def result(self):\n",
    "        '''\n",
    "        Compute the value for the F1 score. Calculates precision and recall, then F1 score.\n",
    "        F1 = 2 * precision * recall / (precision + recall)\n",
    "        :return: F1 score\n",
    "        '''\n",
    "        precision = math_ops.div_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
    "        recall = math_ops.div_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
    "        result = math_ops.div_no_nan(2 * precision * recall, precision + recall)\n",
    "        return result[0] if len(self.thresholds) == 1 else result\n",
    "\n",
    "    def reset_states(self):\n",
    "        '''\n",
    "        Resets all of the metric state variables. Called between epochs, when a metric is evaluated during training.\n",
    "        '''\n",
    "        num_thresholds = len(to_list(self.thresholds))\n",
    "        K.batch_set_value(\n",
    "            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
    "\n",
    "    def get_config(self):\n",
    "        '''\n",
    "        Returns the serializable config of the metric.\n",
    "        :return: serializable config of the metric\n",
    "        '''\n",
    "        config = {\n",
    "            'thresholds': self.init_thresholds,\n",
    "            'top_k': self.top_k,\n",
    "            'class_id': self.class_id\n",
    "        }\n",
    "        base_config = super(F1Score, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D, Conv2D, Flatten, LeakyReLU, BatchNormalization, \\\n",
    "    Activation, concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, ResNet101V2\n",
    "from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "\n",
    "def dcnn_resnet(model_config, input_shape, metrics, n_classes=2, output_bias=None, gpus=1):\n",
    "    '''\n",
    "    Defines a deep convolutional neural network model for multiclass X-ray classification.\n",
    "    :param model_config: A dictionary of parameters associated with the model architecture\n",
    "    :param input_shape: The shape of the model input\n",
    "    :param metrics: Metrics to track model's performance\n",
    "    :return: a Keras Model object with the architecture defined in this method\n",
    "    '''\n",
    "\n",
    "    # Set hyperparameters\n",
    "    nodes_dense0 = model_config['NODES_DENSE0']\n",
    "    lr = model_config['LR']\n",
    "    dropout = model_config['DROPOUT']\n",
    "    l2_lambda = model_config['L2_LAMBDA']\n",
    "    if model_config['OPTIMIZER'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif model_config['OPTIMIZER'] == 'sgd':\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=lr)  # For now, Adam is default option\n",
    "    init_filters = model_config['INIT_FILTERS']\n",
    "    filter_exp_base = model_config['FILTER_EXP_BASE']\n",
    "    conv_blocks = model_config['CONV_BLOCKS']\n",
    "    kernel_size = eval(model_config['KERNEL_SIZE'])\n",
    "    max_pool_size = eval(model_config['MAXPOOL_SIZE'])\n",
    "    strides = eval(model_config['STRIDES'])\n",
    "\n",
    "    # Set output bias\n",
    "    if output_bias is not None:\n",
    "        output_bias = Constant(output_bias)\n",
    "    print(\"MODEL CONFIG: \", model_config)\n",
    "\n",
    "    # Input layer\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "\n",
    "    # Add convolutional (residual) blocks\n",
    "    for i in range(conv_blocks):\n",
    "        X_res = X\n",
    "        X = Conv2D(init_filters * (filter_exp_base ** i), kernel_size, strides=strides, padding='same',\n",
    "                         kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda),\n",
    "                         name='conv' + str(i) + '_0')(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = LeakyReLU()(X)\n",
    "        X = Conv2D(init_filters * (filter_exp_base ** i), kernel_size, strides=strides, padding='same',\n",
    "                         kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda),\n",
    "                         name='conv' + str(i) + '_1')(X)\n",
    "        X = concatenate([X, X_res], name='concat' + str(i))\n",
    "        X = BatchNormalization()(X)\n",
    "        X = LeakyReLU()(X)\n",
    "        X = MaxPool2D(max_pool_size, padding='same')(X)\n",
    "\n",
    "    # Add fully connected layers\n",
    "    X = Flatten()(X)\n",
    "    X = Dropout(dropout)(X)\n",
    "    X = Dense(nodes_dense0, kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda))(X)\n",
    "    X = LeakyReLU()(X)\n",
    "    X = Dense(n_classes, bias_initializer=output_bias)(X)\n",
    "    Y = Activation('softmax', dtype='float32', name='output')(X)\n",
    "\n",
    "    # Set model loss function, optimizer, metrics.\n",
    "    model = Model(inputs=X_input, outputs=Y)\n",
    "    model.summary()\n",
    "    if gpus >= 2:\n",
    "        model = multi_gpu_model(model, gpus=gpus)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import dill\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, CategoricalAccuracy, Precision, Recall, AUC\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "def get_class_weights(histogram, class_multiplier=None):\n",
    "    '''\n",
    "    Computes weights for each class to be applied in the loss function during training.\n",
    "    :param histogram: A list depicting the number of each item in different class\n",
    "    :param class_multiplier: List of values to multiply the calculated class weights by. For further control of class weighting.\n",
    "    :return: A dictionary containing weights for each class\n",
    "    '''\n",
    "    weights = [None] * len(histogram)\n",
    "    for i in range(len(histogram)):\n",
    "        weights[i] = (1.0 / len(histogram)) * sum(histogram) / histogram[i]\n",
    "    class_weight = {i: weights[i] for i in range(len(histogram))}\n",
    "    if class_multiplier is not None:\n",
    "        class_weight = [class_weight[i] * class_multiplier[i] for i in range(len(histogram))]\n",
    "    print(\"Class weights: \", class_weight)\n",
    "    return class_weight\n",
    "\n",
    "\n",
    "def remove_text(img):\n",
    "    '''\n",
    "    Attempts to remove bright textual artifacts from X-ray images. For example, many images indicate the right side of\n",
    "    the body with a white 'R'. Works only for very bright text.\n",
    "    :param img: Numpy array of image\n",
    "    :return: Array of image with (ideally) any characters removed and inpainted\n",
    "    '''\n",
    "    mask = cv2.threshold(img, 230, 255, cv2.THRESH_BINARY)[1][:, :, 0].astype(np.uint8)\n",
    "    img = img.astype(np.uint8)\n",
    "    result = cv2.inpaint(img, mask, 10, cv2.INPAINT_NS).astype(np.float32)\n",
    "    return result\n",
    "\n",
    "def random_minority_oversample(train_set):\n",
    "    '''\n",
    "    Oversample the minority class using the specified algorithm\n",
    "    :param train_set: Training set image file names and labels\n",
    "    :return: A new training set containing oversampled examples\n",
    "    '''\n",
    "    X_train = train_set[[x for x in train_set.columns if x != 'label']].to_numpy()\n",
    "    if X_train.shape[1] == 1:\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "    Y_train = train_set['label'].to_numpy()\n",
    "    sampler = RandomOverSampler(random_state=np.random.randint(0, high=1000))\n",
    "    X_resampled, Y_resampled = sampler.fit_resample(X_train, Y_train)\n",
    "    filenames = X_resampled[:, 1]     # Filename is in second column\n",
    "    label_strs = X_resampled[:, 2]    # Class name is in second column\n",
    "    print(\"Train set shape before oversampling: \", X_train.shape, \" Train set shape after resampling: \", X_resampled.shape)\n",
    "    train_set_resampled = pd.DataFrame({'filename': filenames, 'label': Y_resampled, 'label_str': label_strs})\n",
    "    return train_set_resampled\n",
    "\n",
    "def train_model(cfg, data, callbacks, verbose=1):\n",
    "    '''\n",
    "    Train a and evaluate model on given data.\n",
    "    :param cfg: Project config (from config.yml)\n",
    "    :param data: dict of partitioned dataset\n",
    "    :param callbacks: list of callbacks for Keras model\n",
    "    :param verbose: Verbosity mode to pass to model.fit_generator()\n",
    "    :return: Trained model and associated performance metrics on the test set\n",
    "    '''\n",
    "\n",
    "    # If set in config file, oversample the minority class\n",
    "    if cfg['TRAIN']['IMB_STRATEGY'] == 'random_oversample':\n",
    "        data['TRAIN'] = random_minority_oversample(data['TRAIN'])\n",
    "\n",
    "    # Create ImageDataGenerators\n",
    "    train_img_gen = ImageDataGenerator(rotation_range=10, preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "    val_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "    test_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "\n",
    "    # Create DataFrameIterators\n",
    "    img_shape = tuple(cfg['DATA']['IMG_DIM'])\n",
    "    y_col = 'label_str'\n",
    "    class_mode = 'categorical'\n",
    "    train_generator = train_img_gen.flow_from_dataframe(dataframe=data['TRAIN'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False)\n",
    "    val_generator = val_img_gen.flow_from_dataframe(dataframe=data['VAL'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False)\n",
    "    test_generator = test_img_gen.flow_from_dataframe(dataframe=data['TEST'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False, shuffle=False)\n",
    "\n",
    "    # Save model's ordering of class indices\n",
    "    dill.dump(test_generator.class_indices, open(cfg['PATHS']['OUTPUT_CLASS_INDICES'], 'wb'))\n",
    "\n",
    "    # Apply class imbalance strategy. We have many more X-rays negative for COVID-19 than positive.\n",
    "    histogram = np.bincount(np.array(train_generator.labels).astype(int))  # Get class distribution\n",
    "    class_weight = None\n",
    "    if cfg['TRAIN']['IMB_STRATEGY'] == 'class_weight':\n",
    "        class_multiplier = cfg['TRAIN']['CLASS_MULTIPLIER']\n",
    "        class_multiplier = [class_multiplier[cfg['DATA']['CLASSES'].index(c)] for c in test_generator.class_indices]\n",
    "        class_weight = get_class_weights(histogram, class_multiplier)\n",
    "\n",
    "    # Define metrics.\n",
    "    covid_class_idx = test_generator.class_indices['COVID-19']   # Get index of COVID-19 class\n",
    "    thresholds = 1.0 / len(cfg['DATA']['CLASSES'])      # Binary classification threshold for a class\n",
    "    metrics = [CategoricalAccuracy(name='accuracy'),\n",
    "               Precision(name='precision', thresholds=thresholds, class_id=covid_class_idx),\n",
    "               Recall(name='recall', thresholds=thresholds, class_id=covid_class_idx),\n",
    "               AUC(name='auc'),\n",
    "               F1Score(name='f1score', thresholds=thresholds, class_id=covid_class_idx)]\n",
    "\n",
    "    # Define the model.\n",
    "    print('Training distribution: ', ['Class ' + list(test_generator.class_indices.keys())[i] + ': ' + str(histogram[i]) + '. '\n",
    "           for i in range(len(histogram))])\n",
    "    input_shape = cfg['DATA']['IMG_DIM'] + [3]\n",
    "    num_gpus = cfg['TRAIN']['NUM_GPUS']\n",
    "    model_def = dcnn_resnet\n",
    "\n",
    "    # if cfg['TRAIN']['CLASS_MODE'] == 'binary':\n",
    "    #    histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
    "    #    output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
    "    #    model = model_def(cfg['NN']['DCNN_BINARY'], input_shape, metrics, 2, output_bias=output_bias, gpus=num_gpus)\n",
    "    #else:\n",
    "    #    n_classes = len(cfg['DATA']['CLASSES'])\n",
    "    #    histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
    "    #    output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
    "    #    model = model_def(cfg['NN']['DCNN_MULTICLASS'], input_shape, metrics, n_classes, output_bias=output_bias,\n",
    "    #                      gpus=num_gpus)\n",
    "                    \n",
    "\n",
    "    histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
    "    output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
    "    model = model_def(cfg['NN']['DCNN_BINARY'], input_shape, metrics, 2, output_bias=output_bias, gpus=num_gpus)\n",
    "\n",
    "    # Train the model.\n",
    "    steps_per_epoch = ceil(train_generator.n / train_generator.batch_size)\n",
    "    val_steps = ceil(val_generator.n / val_generator.batch_size)\n",
    "    history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=cfg['TRAIN']['EPOCHS'],\n",
    "                                  validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks,\n",
    "                                  verbose=verbose)\n",
    "                           \n",
    "\n",
    "    # Run the model on the test set and print the resulting performance metrics.\n",
    "    test_results = model.evaluate_generator(test_generator, verbose=1)\n",
    "    test_metrics = {}\n",
    "    test_summary_str = [['**Metric**', '**Value**']]\n",
    "    for metric, value in zip(model.metrics_names, test_results):\n",
    "        test_metrics[metric] = value\n",
    "        print(metric, ' = ', value)\n",
    "        test_summary_str.append([metric, str(value)])\n",
    "    return model, test_metrics, test_generator\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1489 non-validated image filenames belonging to 2 classes.\n",
      "Found 146 non-validated image filenames belonging to 2 classes.\n",
      "Found 182 non-validated image filenames belonging to 2 classes.\n",
      "Class weights:  [26.589285714285715, 0.07643737166324435]\n",
      "Training distribution:  ['Class COVID-19: 28. ', 'Class non-COVID-19: 1461. ']\n",
      "MODEL CONFIG:  {'KERNEL_SIZE': '(3,3)', 'STRIDES': '(1,1)', 'INIT_FILTERS': 16, 'FILTER_EXP_BASE': 3, 'MAXPOOL_SIZE': '(2,2)', 'CONV_BLOCKS': 3, 'NODES_DENSE0': 128, 'LR': 1e-05, 'OPTIMIZER': 'adam', 'DROPOUT': 0.4, 'L2_LAMBDA': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 20:59:46.311641: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-01 20:59:46.314610: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_0 (Conv2D)                (None, 224, 224, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 224, 224, 16) 64          conv0_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 224, 224, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv0_1 (Conv2D)                (None, 224, 224, 16) 2320        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat0 (Concatenate)           (None, 224, 224, 19) 0           conv0_1[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 224, 224, 19) 76          concat0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 224, 224, 19) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 112, 112, 19) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1_0 (Conv2D)                (None, 112, 112, 48) 8256        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 112, 112, 48) 192         conv1_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 112, 112, 48) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 112, 112, 48) 20784       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concat1 (Concatenate)           (None, 112, 112, 67) 0           conv1_1[0][0]                    \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 112, 112, 67) 268         concat1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 112, 112, 67) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 67)   0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_0 (Conv2D)                (None, 56, 56, 144)  86976       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 144)  576         conv2_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 144)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                (None, 56, 56, 144)  186768      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concat2 (Concatenate)           (None, 56, 56, 211)  0           conv2_1[0][0]                    \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 56, 56, 211)  844         concat2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 56, 56, 211)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 211)  0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 165424)       0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 165424)       0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          21174400    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Activation)             (None, 2)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,482,230\n",
      "Trainable params: 21,481,220\n",
      "Non-trainable params: 1,010\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 20:59:48.363414: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 214s 5s/step - loss: 461.9993 - accuracy: 0.9080 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9247 - f1score: 0.0000e+00 - val_loss: 271.5365 - val_accuracy: 0.9384 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9702 - val_f1score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 197s 4s/step - loss: 431.0725 - accuracy: 0.9781 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9836 - f1score: 0.0000e+00 - val_loss: 246.7792 - val_accuracy: 0.4589 - val_precision: 0.0128 - val_recall: 0.3333 - val_auc: 0.4498 - val_f1score: 0.0247\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 193s 4s/step - loss: 404.7710 - accuracy: 0.9611 - precision: 0.1224 - recall: 0.1007 - auc: 0.9832 - f1score: 0.1102 - val_loss: 243.7573 - val_accuracy: 0.9452 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9699 - val_f1score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 197s 4s/step - loss: 382.8649 - accuracy: 0.9724 - precision: 0.2867 - recall: 0.1445 - auc: 0.9882 - f1score: 0.1813 - val_loss: 249.4145 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9773 - val_f1score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 195s 4s/step - loss: 364.7706 - accuracy: 0.9623 - precision: 0.1075 - recall: 0.0842 - auc: 0.9769 - f1score: 0.0928 - val_loss: 258.0153 - val_accuracy: 0.9589 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 198s 4s/step - loss: 350.4739 - accuracy: 0.9819 - precision: 0.4812 - recall: 0.3957 - auc: 0.9910 - f1score: 0.4335 - val_loss: 268.3102 - val_accuracy: 0.9726 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9719 - val_f1score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 202s 4s/step - loss: 335.1298 - accuracy: 0.9881 - precision: 0.6785 - recall: 0.5144 - auc: 0.9952 - f1score: 0.5745 - val_loss: 277.8178 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9785 - val_f1score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 194s 4s/step - loss: 323.2394 - accuracy: 0.9852 - precision: 0.4147 - recall: 0.2529 - auc: 0.9933 - f1score: 0.3097 - val_loss: 285.4857 - val_accuracy: 0.9658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9749 - val_f1score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 207s 4s/step - loss: 312.5739 - accuracy: 0.9888 - precision: 0.6926 - recall: 0.4534 - auc: 0.9969 - f1score: 0.5433 - val_loss: 290.5333 - val_accuracy: 0.9521 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9698 - val_f1score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 197s 4s/step - loss: 301.5712 - accuracy: 0.9918 - precision: 0.8105 - recall: 0.7692 - auc: 0.9975 - f1score: 0.7877 - val_loss: 291.7515 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9789 - val_f1score: 0.0000e+00\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianbfan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 1s/step - loss: 248.0834 - accuracy: 0.9505 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9841 - f1score: 0.0000e+00\n",
      "loss  =  248.08340454101562\n",
      "accuracy  =  0.9505494236946106\n",
      "precision  =  0.0\n",
      "recall  =  0.0\n",
      "auc  =  0.9841203093528748\n",
      "f1score  =  0.0\n"
     ]
    }
   ],
   "source": [
    "model, test_metrics, test_generator = train_model(cfg, data, callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "import datetime\n",
    "\n",
    "cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "model_path = cfg['PATHS']['MODEL_WEIGHTS'] + 'model' + cur_date + '.h5'\n",
    "save_model(model, model_path)  # Save the model's weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
